{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando os dados\n",
    "\n",
    "Praticamente todos os dados são categóricos, portanto algum método de vetorização\n",
    "deve ser necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/sales.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>SECTOR1_ID</th>\n",
       "      <th>SECTOR2_ID</th>\n",
       "      <th>SECTOR3_ID</th>\n",
       "      <th>SECTOR4_ID</th>\n",
       "      <th>SALES_QUANTITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>41.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>46.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  WEEK  PRODUCT_ID  SECTOR1_ID  SECTOR2_ID  SECTOR3_ID  SECTOR4_ID  \\\n",
       "0  2017     7           1           1           1           1           1   \n",
       "1  2015    27           2           1           2           2           2   \n",
       "2  2015    24           3           2           3           3           3   \n",
       "3  2016    36           4           3           4           4           4   \n",
       "4  2016     6           5           4           5           5           5   \n",
       "\n",
       "   SALES_QUANTITY  \n",
       "0           14.00  \n",
       "1           41.00  \n",
       "2           10.46  \n",
       "3           46.00  \n",
       "4            5.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>SECTOR1_ID</th>\n",
       "      <th>SECTOR2_ID</th>\n",
       "      <th>SECTOR3_ID</th>\n",
       "      <th>SECTOR4_ID</th>\n",
       "      <th>SALES_QUANTITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>442573.000000</td>\n",
       "      <td>442573.000000</td>\n",
       "      <td>442573.000000</td>\n",
       "      <td>442573.000000</td>\n",
       "      <td>442573.000000</td>\n",
       "      <td>442573.000000</td>\n",
       "      <td>442573.000000</td>\n",
       "      <td>442573.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2015.800880</td>\n",
       "      <td>27.532669</td>\n",
       "      <td>3481.469276</td>\n",
       "      <td>2.415721</td>\n",
       "      <td>5.473027</td>\n",
       "      <td>26.874136</td>\n",
       "      <td>61.164072</td>\n",
       "      <td>17.953565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.632561</td>\n",
       "      <td>15.512948</td>\n",
       "      <td>2202.165928</td>\n",
       "      <td>1.363241</td>\n",
       "      <td>4.058683</td>\n",
       "      <td>21.175039</td>\n",
       "      <td>49.865031</td>\n",
       "      <td>58.916102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1615.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2016.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>3282.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2016.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>5157.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>9372.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>1891.265000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                YEAR           WEEK     PRODUCT_ID     SECTOR1_ID  \\\n",
       "count  442573.000000  442573.000000  442573.000000  442573.000000   \n",
       "mean     2015.800880      27.532669    3481.469276       2.415721   \n",
       "std         0.632561      15.512948    2202.165928       1.363241   \n",
       "min      2015.000000       1.000000       1.000000       1.000000   \n",
       "25%      2015.000000      12.000000    1615.000000       1.000000   \n",
       "50%      2016.000000      29.000000    3282.000000       2.000000   \n",
       "75%      2016.000000      41.000000    5157.000000       4.000000   \n",
       "max      2017.000000      52.000000    9372.000000       7.000000   \n",
       "\n",
       "          SECTOR2_ID     SECTOR3_ID     SECTOR4_ID  SALES_QUANTITY  \n",
       "count  442573.000000  442573.000000  442573.000000   442573.000000  \n",
       "mean        5.473027      26.874136      61.164072       17.953565  \n",
       "std         4.058683      21.175039      49.865031       58.916102  \n",
       "min         1.000000       1.000000       1.000000        0.005000  \n",
       "25%         1.000000      10.000000      18.000000        2.000000  \n",
       "50%         6.000000      20.000000      48.000000        6.000000  \n",
       "75%         8.000000      39.000000      97.000000       14.000000  \n",
       "max        22.000000     104.000000     222.000000     1891.265000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando treino, validação e teste\n",
    "\n",
    "Os dados de teste será os da semana 11 do ano 2017, o resto dos dados será dividido\n",
    "aleatóriamente em treino e validão na proporção 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indexes = set(df.index)\n",
    "test_indexes = set(df.loc[(df['YEAR'] == 2017) & (df['WEEK'] == 11)].index)\n",
    "remaining_indexes = all_indexes - test_indexes\n",
    "\n",
    "random.seed(42)\n",
    "val_indexes = set(random.sample(remaining_indexes, \n",
    "    int(len(remaining_indexes)*0.20)))\n",
    "\n",
    "train_indexes = remaining_indexes - val_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442573 4891 350146 87536\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "print(len(all_indexes), len(test_indexes), len(train_indexes), len(val_indexes))\n",
    "print(train_indexes.isdisjoint(test_indexes))\n",
    "print(train_indexes.isdisjoint(val_indexes))\n",
    "print(test_indexes.isdisjoint(val_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.filter(items = list(train_indexes), axis=0)\n",
    "val_data = df.filter(items = list(val_indexes), axis=0)\n",
    "test_data = df.filter(items = list(test_indexes), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['SALES_QUANTITY']\n",
    "y_val = val_data['SALES_QUANTITY']\n",
    "y_test = test_data['SALES_QUANTITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[train_data.columns[:-1]]\n",
    "X_val = val_data[val_data.columns[:-1]]\n",
    "X_test = test_data[test_data.columns[:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Baseline\n",
    "\n",
    "Vamos treinar um modelo random forest sem nenhuma transformação nos dados para \n",
    "ver o que acontece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(max_depth=10, random_state=42)\n",
    "rf.fit(np.array(X_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.06549944565198\n",
      "12.530874160613662\n",
      "\n",
      "\n",
      "32.59756814812514\n",
      "12.555507610129522\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mean_squared_error(np.array(y_test), \n",
    "    rf.predict(np.array(X_test)))))\n",
    "\n",
    "print(mean_absolute_error(np.array(y_test), rf.predict(np.array(X_test))))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(np.sqrt(mean_squared_error(np.array(y_val), \n",
    "    rf.predict(np.array(X_val)))))\n",
    "\n",
    "print(mean_absolute_error(np.array(y_val), rf.predict(np.array(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding para lidar com as variáveis categóricas\n",
    "\n",
    "Como as classes são muitas (~10k), a dimensionalidade do problema aumentará\n",
    "exponencialmente.\n",
    "\n",
    "Por isso vamos usar também o método para representações esparsas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_df(df, columns):\n",
    "    labeler = LabelEncoder()\n",
    "    onehot = OneHotEncoder(sparse=True)\n",
    "\n",
    "    empty_array = np.empty((len(df), 0), int)\n",
    "    for column in columns:\n",
    "        numeric_data = labeler.fit_transform(df[column])\n",
    "        numeric_data = numeric_data.reshape(len(df), 1)\n",
    "        empty_array = np.append(empty_array, numeric_data, 1)\n",
    "\n",
    "        data = onehot.fit_transform(empty_array)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encode_df(df.copy(), list(df.columns)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotrf = RandomForestRegressor(max_depth=10, random_state=42)\n",
    "hotrf.fit(x[list(train_indexes)], np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.90881422696889\n",
      "15.52213065327469\n",
      "\n",
      "\n",
      "37.18737867843924\n",
      "15.542937799229975\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mean_squared_error(np.array(y_test), \n",
    "    hotrf.predict(x[list(test_indexes)]))))\n",
    "\n",
    "print(mean_absolute_error(np.array(y_test), \n",
    "    hotrf.predict(x[list(test_indexes)])))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(np.sqrt(mean_squared_error(np.array(y_val), \n",
    "    hotrf.predict(x[list(val_indexes)]))))\n",
    "\n",
    "print(mean_absolute_error(np.array(y_val), \n",
    "    hotrf.predict(x[list(val_indexes)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível notar uma piora no resultado. Deve ser investigada, talvez o modelo\n",
    "esparso precise de mais iterações, talvez a esparsidade piore o problema.\n",
    "\n",
    "Testamos também a redução de dimensionalidade porém ficará omitida aqui pois \n",
    "o algoritmo do RandomForest não convergiu com o critério de parada estabelecido.\n",
    "\n",
    "Vamos agora testar o LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "\n",
    "Vamos testar o LGBM primeiramente sem nenhuma transformação nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 667\n",
      "[LightGBM] [Info] Number of data points in the train set: 350146, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 18.060209\n",
      "[1]\tvalid_0's rmse: 54.9403\n",
      "[2]\tvalid_0's rmse: 53.1277\n",
      "[3]\tvalid_0's rmse: 51.5291\n",
      "[4]\tvalid_0's rmse: 50.1452\n",
      "[5]\tvalid_0's rmse: 49.115\n",
      "[6]\tvalid_0's rmse: 48.0521\n",
      "[7]\tvalid_0's rmse: 47.2446\n",
      "[8]\tvalid_0's rmse: 46.6118\n",
      "[9]\tvalid_0's rmse: 45.9305\n",
      "[10]\tvalid_0's rmse: 45.3494\n",
      "[11]\tvalid_0's rmse: 44.8644\n",
      "[12]\tvalid_0's rmse: 44.4052\n",
      "[13]\tvalid_0's rmse: 43.962\n",
      "[14]\tvalid_0's rmse: 43.4732\n",
      "[15]\tvalid_0's rmse: 43.1078\n",
      "[16]\tvalid_0's rmse: 42.7598\n",
      "[17]\tvalid_0's rmse: 42.54\n",
      "[18]\tvalid_0's rmse: 42.2355\n",
      "[19]\tvalid_0's rmse: 41.9229\n",
      "[20]\tvalid_0's rmse: 41.6698\n",
      "[21]\tvalid_0's rmse: 41.4772\n",
      "[22]\tvalid_0's rmse: 41.209\n",
      "[23]\tvalid_0's rmse: 41.0248\n",
      "[24]\tvalid_0's rmse: 40.8865\n",
      "[25]\tvalid_0's rmse: 40.7069\n",
      "[26]\tvalid_0's rmse: 40.4893\n",
      "[27]\tvalid_0's rmse: 40.3041\n",
      "[28]\tvalid_0's rmse: 40.209\n",
      "[29]\tvalid_0's rmse: 40.0477\n",
      "[30]\tvalid_0's rmse: 39.9457\n",
      "[31]\tvalid_0's rmse: 39.8123\n",
      "[32]\tvalid_0's rmse: 39.6606\n",
      "[33]\tvalid_0's rmse: 39.6222\n",
      "[34]\tvalid_0's rmse: 39.5846\n",
      "[35]\tvalid_0's rmse: 39.5349\n",
      "[36]\tvalid_0's rmse: 39.4528\n",
      "[37]\tvalid_0's rmse: 39.4055\n",
      "[38]\tvalid_0's rmse: 39.3256\n",
      "[39]\tvalid_0's rmse: 39.234\n",
      "[40]\tvalid_0's rmse: 39.1826\n",
      "[41]\tvalid_0's rmse: 39.1017\n",
      "[42]\tvalid_0's rmse: 39.0436\n",
      "[43]\tvalid_0's rmse: 38.9781\n",
      "[44]\tvalid_0's rmse: 38.8881\n",
      "[45]\tvalid_0's rmse: 38.8286\n",
      "[46]\tvalid_0's rmse: 38.7783\n",
      "[47]\tvalid_0's rmse: 38.709\n",
      "[48]\tvalid_0's rmse: 38.6777\n",
      "[49]\tvalid_0's rmse: 38.637\n",
      "[50]\tvalid_0's rmse: 38.5844\n"
     ]
    }
   ],
   "source": [
    "lgbm_train = lgb.Dataset(np.array(X_train), label=np.array(y_train))\n",
    "lgbm_val = lgb.Dataset(np.array(X_val), \n",
    "    reference=lgbm_train, \n",
    "    label=np.array(y_val))\n",
    "\n",
    "param = {'num_leaves': 31, 'objective': 'regression'}\n",
    "param['metric'] = 'RMSE'\n",
    "\n",
    "num_round = 50\n",
    "bst = lgb.train(param, lgbm_train, num_round, valid_sets=[lgbm_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.59249450747933\n",
      "13.967531591180608\n"
     ]
    }
   ],
   "source": [
    "ypred = bst.predict(np.array(X_test))\n",
    "\n",
    "print(np.sqrt(mean_squared_error(np.array(y_test), np.array(ypred))))\n",
    "\n",
    "print(mean_absolute_error(np.array(y_test), np.array(ypred)))\n",
    "\n",
    "#Ainda abaixo do baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redução de dimensionalidade com SVD\n",
    "\n",
    "Vamos fazer one-hot encoding dos dados e aplicar Truncated SVD nos dados \n",
    "para reduzir as dimensões do problema.\n",
    "\n",
    "Seria interessante podermos testar também tSNE ou PCA, porém a esparsidade \n",
    "não nos permite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=1000, random_state=42)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vamos usar 1000 componentes, o que representa uma redução de cerca de 90%.\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000, n_iter=5, random_state=42)\n",
    "svd.fit(x[list(train_indexes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8644461818045971\n"
     ]
    }
   ],
   "source": [
    "#Reduzimos a dimensão do problema em cerca de 90% e ainda temos 86% de sua \n",
    "#representatividade\n",
    "print(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.420694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255000\n",
      "[LightGBM] [Info] Number of data points in the train set: 350146, number of used features: 1000\n",
      "[LightGBM] [Info] Start training from score 18.060209\n",
      "[1]\tvalid_0's rmse: 53.1767\n",
      "[2]\tvalid_0's rmse: 49.7245\n",
      "[3]\tvalid_0's rmse: 46.6791\n",
      "[4]\tvalid_0's rmse: 43.9142\n",
      "[5]\tvalid_0's rmse: 41.603\n",
      "[6]\tvalid_0's rmse: 39.5703\n",
      "[7]\tvalid_0's rmse: 37.8434\n",
      "[8]\tvalid_0's rmse: 36.3044\n",
      "[9]\tvalid_0's rmse: 34.9241\n",
      "[10]\tvalid_0's rmse: 33.6658\n",
      "[11]\tvalid_0's rmse: 32.6059\n",
      "[12]\tvalid_0's rmse: 31.6981\n",
      "[13]\tvalid_0's rmse: 30.8577\n",
      "[14]\tvalid_0's rmse: 30.1428\n",
      "[15]\tvalid_0's rmse: 29.5226\n",
      "[16]\tvalid_0's rmse: 29.0099\n",
      "[17]\tvalid_0's rmse: 28.5402\n",
      "[18]\tvalid_0's rmse: 28.1198\n",
      "[19]\tvalid_0's rmse: 27.7452\n",
      "[20]\tvalid_0's rmse: 27.4322\n",
      "[21]\tvalid_0's rmse: 27.1165\n",
      "[22]\tvalid_0's rmse: 26.8438\n",
      "[23]\tvalid_0's rmse: 26.628\n",
      "[24]\tvalid_0's rmse: 26.3646\n",
      "[25]\tvalid_0's rmse: 26.165\n",
      "[26]\tvalid_0's rmse: 25.9904\n",
      "[27]\tvalid_0's rmse: 25.8335\n",
      "[28]\tvalid_0's rmse: 25.6698\n",
      "[29]\tvalid_0's rmse: 25.5003\n",
      "[30]\tvalid_0's rmse: 25.376\n",
      "[31]\tvalid_0's rmse: 25.2311\n",
      "[32]\tvalid_0's rmse: 25.1161\n",
      "[33]\tvalid_0's rmse: 25.0201\n",
      "[34]\tvalid_0's rmse: 24.9227\n",
      "[35]\tvalid_0's rmse: 24.8034\n",
      "[36]\tvalid_0's rmse: 24.7081\n",
      "[37]\tvalid_0's rmse: 24.648\n",
      "[38]\tvalid_0's rmse: 24.5889\n",
      "[39]\tvalid_0's rmse: 24.5436\n",
      "[40]\tvalid_0's rmse: 24.462\n",
      "[41]\tvalid_0's rmse: 24.374\n",
      "[42]\tvalid_0's rmse: 24.2914\n",
      "[43]\tvalid_0's rmse: 24.2035\n",
      "[44]\tvalid_0's rmse: 24.1384\n",
      "[45]\tvalid_0's rmse: 24.0821\n",
      "[46]\tvalid_0's rmse: 23.9866\n",
      "[47]\tvalid_0's rmse: 23.9522\n",
      "[48]\tvalid_0's rmse: 23.8873\n",
      "[49]\tvalid_0's rmse: 23.8224\n",
      "[50]\tvalid_0's rmse: 23.7653\n"
     ]
    }
   ],
   "source": [
    "lgbm_train = lgb.Dataset(svd.transform(x[list(train_indexes)]),\n",
    "    label=np.array(y_train))\n",
    "\n",
    "lgbm_val = lgb.Dataset(svd.transform((x[list(val_indexes)])), \n",
    "    reference=lgbm_train, \n",
    "    label=np.array(y_val))\n",
    "\n",
    "param = {'num_leaves': 31, 'objective': 'regression'}\n",
    "param['metric'] = 'RMSE'\n",
    "\n",
    "num_round = 50\n",
    "bst = lgb.train(param, lgbm_train, num_round, valid_sets=[lgbm_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.016153433833722\n",
      "10.496038888902202\n"
     ]
    }
   ],
   "source": [
    "ypred = bst.predict(svd.transform(x[list(test_indexes)]))\n",
    "\n",
    "print(np.sqrt(mean_squared_error(np.array(y_test), np.array(ypred))))\n",
    "\n",
    "print(mean_absolute_error(np.array(y_test), np.array(ypred)))\n",
    "\n",
    "#Agora sim houve uma melhora tanto no RMSE quanto no MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8268\n",
      "[LightGBM] [Info] Number of data points in the train set: 350146, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 18.060209\n",
      "[1]\tvalid_0's rmse: 52.4883\n",
      "[2]\tvalid_0's rmse: 48.289\n",
      "[3]\tvalid_0's rmse: 44.5368\n",
      "[4]\tvalid_0's rmse: 41.2152\n",
      "[5]\tvalid_0's rmse: 38.3708\n",
      "[6]\tvalid_0's rmse: 35.8971\n",
      "[7]\tvalid_0's rmse: 33.6755\n",
      "[8]\tvalid_0's rmse: 31.8108\n",
      "[9]\tvalid_0's rmse: 30.2171\n",
      "[10]\tvalid_0's rmse: 28.85\n",
      "[11]\tvalid_0's rmse: 27.7067\n",
      "[12]\tvalid_0's rmse: 26.7079\n",
      "[13]\tvalid_0's rmse: 25.8291\n",
      "[14]\tvalid_0's rmse: 25.0873\n",
      "[15]\tvalid_0's rmse: 24.4561\n",
      "[16]\tvalid_0's rmse: 23.9068\n",
      "[17]\tvalid_0's rmse: 23.4007\n",
      "[18]\tvalid_0's rmse: 22.9974\n",
      "[19]\tvalid_0's rmse: 22.6672\n",
      "[20]\tvalid_0's rmse: 22.3681\n",
      "[21]\tvalid_0's rmse: 22.1448\n",
      "[22]\tvalid_0's rmse: 21.945\n",
      "[23]\tvalid_0's rmse: 21.7686\n",
      "[24]\tvalid_0's rmse: 21.6432\n",
      "[25]\tvalid_0's rmse: 21.5199\n",
      "[26]\tvalid_0's rmse: 21.4407\n",
      "[27]\tvalid_0's rmse: 21.3018\n",
      "[28]\tvalid_0's rmse: 21.1717\n",
      "[29]\tvalid_0's rmse: 21.1367\n",
      "[30]\tvalid_0's rmse: 21.0072\n",
      "[31]\tvalid_0's rmse: 20.8752\n",
      "[32]\tvalid_0's rmse: 20.7912\n",
      "[33]\tvalid_0's rmse: 20.7175\n",
      "[34]\tvalid_0's rmse: 20.639\n",
      "[35]\tvalid_0's rmse: 20.552\n",
      "[36]\tvalid_0's rmse: 20.484\n",
      "[37]\tvalid_0's rmse: 20.4296\n",
      "[38]\tvalid_0's rmse: 20.4048\n",
      "[39]\tvalid_0's rmse: 20.3796\n",
      "[40]\tvalid_0's rmse: 20.3337\n",
      "[41]\tvalid_0's rmse: 20.2824\n",
      "[42]\tvalid_0's rmse: 20.2298\n",
      "[43]\tvalid_0's rmse: 20.2054\n",
      "[44]\tvalid_0's rmse: 20.1669\n",
      "[45]\tvalid_0's rmse: 20.1667\n",
      "[46]\tvalid_0's rmse: 20.1606\n",
      "[47]\tvalid_0's rmse: 20.1432\n",
      "[48]\tvalid_0's rmse: 20.1052\n",
      "[49]\tvalid_0's rmse: 20.0857\n",
      "[50]\tvalid_0's rmse: 20.0633\n"
     ]
    }
   ],
   "source": [
    "lgbm_train = lgb.Dataset(np.array(X_train), label=np.array(y_train))\n",
    "lgbm_val = lgb.Dataset(np.array(X_val), \n",
    "    reference=lgbm_train, \n",
    "    label=np.array(y_val))\n",
    "\n",
    "param = {'num_leaves': 31, 'objective': 'regression'}\n",
    "param['metric'] = 'RMSE'\n",
    "\n",
    "num_round = 50\n",
    "bst = lgb.train(param, lgbm_train, num_round, valid_sets=[lgbm_val], \n",
    "    categorical_feature=[item for item in range(len(X_train.columns))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.163811687037494\n",
      "7.54099396515019\n"
     ]
    }
   ],
   "source": [
    "ypred = bst.predict(np.array(X_test))\n",
    "\n",
    "print(np.sqrt(mean_squared_error(np.array(y_test), np.array(ypred))))\n",
    "\n",
    "print(mean_absolute_error(np.array(y_test), np.array(ypred)))\n",
    "\n",
    "#Melhor modelo encontrado até então, inclusive em questão de tempo de \n",
    "#implementação e treino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Próximos Passos\n",
    "\n",
    "Tomar como base o melhor modelo e fazer uma busca dos hiperparâmetros que\n",
    "otimizem o problema.\n",
    "\n",
    "Além disso, previsão de demanda parece ser um bom problema para ser resolvido \n",
    "com séries temporais (essas não tenho familiaridade até então) ou talvez com\n",
    "RNNs."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d3d90c622898854b2ab6931f6aedd7af87d80f456fa2216af73a06a9f0b3dce"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('hortifruti_movies')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
